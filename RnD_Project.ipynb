{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78583ea",
   "metadata": {},
   "source": [
    "### Research \\& Development Project\n",
    "___\n",
    "This is the step-by-step guide to what I did during the project. Following these steps will provide the same results\n",
    "at which I arrived.<br>\n",
    "#### Part 1: Setup\n",
    "* __Step 1.1__: Setup necessary parameters, create the virtual environment, install necessary packages.<br>\n",
    "Once this is done, set the kernel to said environment.<br>\n",
    "For this to work, follow the instructions in the README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97093c9",
   "metadata": {},
   "source": [
    "* __Step 1.2__: Load all libraries that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36025129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForSequenceClassification as amfsc, \n",
    "                         AutoTokenizer as at, \n",
    "                         AutoModelForCausalLM)\n",
    "from accelerate import Accelerator\n",
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "import sacrebleu\n",
    "import torch\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a3ad3",
   "metadata": {},
   "source": [
    "* __Step 1.3__: Retrieve and convert OpenNMT's NLLB transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d795252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the model from https://www.opennmt.net/Models-py/\n",
    "!wget https://s3.amazonaws.com/opennmt-models/nllb-200/nllb-200-3.3B-onmt.pt\n",
    "!wget https://s3.amazonaws.com/opennmt-models/nllb-200/flores200_sacrebleu_tokenizer_spm.model\n",
    "!mkdir nmt_resources\n",
    "!mv nllb-200-3.3B-onmt.pt flores200_sacrebleu_tokenizer_spm.model nmt_resources/\n",
    "!ct2-opennmt-py-converter --model nmt_resources/nllb-200-3.3B-onmt.pt --quantization int8 --output_dir nmt_resources/nllb-200-3.3B-int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a53c78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 594751 on cluster pelle\n"
     ]
    }
   ],
   "source": [
    "!sbatch execute.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ee3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "            594751       gpu execute. mame0175 PD       0:00      1 (Priority)\n"
     ]
    }
   ],
   "source": [
    "#TEMPORARY CELL! Remove before handing in project!\n",
    "!squeue -u mame0175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c6a474",
   "metadata": {},
   "source": [
    "#### Part 2: Translation\n",
    "___\n",
    "The following chunks of code will load the models, translate the source text, and store the translated target text.\n",
    "* __Step 2.1__: Load NMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1baf52",
   "metadata": {
    "tags": [
     "load NMT"
    ]
   },
   "outputs": [],
   "source": [
    "#--------------------Part 2.1: LOAD NMT--------------------#\n",
    "''' Test for GPU and load translator '''\n",
    "\n",
    "# 2.1.0: Set device to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2.1.1: Instantiate the translator\n",
    "ct_model_path = 'nmt_resources/nllb-200-3.3B-int8'\n",
    "sp_model_path = 'nmt_resources/flores200_sacrebleu_tokenizer_spm.model'\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "translator = ctranslate2.Translator(ct_model_path, device)\n",
    "\n",
    "print(device)\n",
    "print(ctranslate2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e8786",
   "metadata": {},
   "source": [
    "* __Step 2.2__: Load LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3d07e",
   "metadata": {
    "tags": [
     "load LLM"
    ]
   },
   "outputs": [],
   "source": [
    "#--------------------Part 2.2: LOAD LLM--------------------#\n",
    "''' Test for GPU and load the model '''\n",
    "\n",
    "# 2.2.0: Set device to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2.2.1: Instantiate tokenizer and model\n",
    "tokenizer = at.from_pretrained('meta-llama/Llama-2-13b-chat-hf')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-13b-hf',\n",
    "    device_map='auto',  # sets to device='cuda' if eligible\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "resrv_mem = torch.cuda.memory.memory_reserved(0)\n",
    "alloc_mem = torch.cuda.memory.memory_allocated(0)\n",
    "print(f\"\\n Reserved memory: {resrv_mem / 1024**3:.2f} GB\\n\",\n",
    "      f\"Allocated memory: {alloc_mem / 1024**3:.2f} GB\\n\",\n",
    "      f\"Unused memory: {(resrv_mem - alloc_mem) / 1024**3:.2f} GB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba604413",
   "metadata": {},
   "source": [
    "* __Part 2.3__: Translate source text using the NMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd807a6e",
   "metadata": {
    "tags": [
     "run NMT"
    ]
   },
   "outputs": [],
   "source": [
    "#--------------------PARt 2.3: NMT--------------------#\n",
    "''' Encode input, translate source text, decode output '''\n",
    "\n",
    "# 2.3.0 Define source and target languages\n",
    "src_lang = 'eng_Latn'\n",
    "tgt_lang = 'deu_Latn'\n",
    "\n",
    "# 2.3.1: Define paths\n",
    "inp_path = 'paraphrases/eng_Latn.txt'\n",
    "out_path = 'translations/nmt_Latn.de'\n",
    "\n",
    "beam_size = 4\n",
    "\n",
    "# 2.3.2: Open and read input file; return lines\n",
    "with open(inp_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 2.3.3: Define source and target prefixes\n",
    "source_sents = [line.strip() for line in lines]\n",
    "print(src_lang, source_sents[0], sep=' --> ')\n",
    "target_prefix = [[tgt_lang]] * len(source_sents)\n",
    "\n",
    "# 2.3.4: Subword source sentences\n",
    "source_sents_subword = sp.encode_as_pieces(source_sents)\n",
    "source_sents_subword = [[src_lang] + sent + ['</s>'] for sent in source_sents_subword]\n",
    "\n",
    "# 2.3.5: Translate source sentences\n",
    "translator = ctranslate2.Translator(ct_model_path, device=device)\n",
    "translations = translator.translate_batch(source_sents_subword, \n",
    "                                          batch_type='tokens',\n",
    "                                          max_batch_size=2048,\n",
    "                                          beam_size=beam_size,\n",
    "                                          target_prefix=target_prefix)\n",
    "translations = [translation.hypotheses[0] for translation in translations]\n",
    "\n",
    "# 2.3.6: Desubword target sentences\n",
    "translations_desubword = sp.decode(translations)\n",
    "translations_desubword = [sent[len(tgt_lang):].strip() for sent in translations_desubword]\n",
    "print(tgt_lang, translations_desubword[0], sep=' --> ')\n",
    "\n",
    "# 2.3.7: Create target directory\n",
    "try:\n",
    "    os.mkdir('translations')\n",
    "except FileExistsError:\n",
    "    print('Directory already exists. Proceeds as normal.')\n",
    "    pass\n",
    "\n",
    "# 2.3.8: Write translation to output file\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for line in translations_desubword:\n",
    "        f.write(line.strip() + '\\n')\n",
    "print(f'Translations successfully executed and saved to: {out_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25685d15",
   "metadata": {},
   "source": [
    "* __Part 2.4__: Translate the source text using the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f437c5f",
   "metadata": {
    "tags": [
     "run LLM"
    ]
   },
   "outputs": [],
   "source": [
    "#--------------------PART 2.4: LLM--------------------#\n",
    "''' Formalize prompt, set temperature, translate source text '''\n",
    "\n",
    "# 2.4.0 Define source and target languages\n",
    "src_lang = 'English'\n",
    "tgt_lang = 'German'\n",
    "\n",
    "# 2.4.1: Define paths\n",
    "inp_path = 'paraphrases/eng_Latn.txt'\n",
    "out_path = f'translations/llm_t{sys.argv[2]}_Latn.de'\n",
    "\n",
    "# 2.4.2 Load source text\n",
    "with open(inp_path, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "\n",
    "# 2.4.3: Create target directory\n",
    "try:\n",
    "    os.mkdir('translations')\n",
    "except FileExistsError:\n",
    "    print('Directory already exists. Proceeds as normal.')\n",
    "    pass\n",
    "\n",
    "# 2.4.4: Set batch size and provide prompt to model\n",
    "batch_size = 1     # 10 was chosen through testing different batch sizes, in which higher numbers failed to be translated\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(0, 100, batch_size):\n",
    "        batch = ''\n",
    "        for line in lines[i:i+batch_size]:\n",
    "            batch += line + '\\n'\n",
    "        prompt = f'''\n",
    "                  Translate the following text from {src_lang} to {tgt_lang}.\n",
    "                  Output exactly the same number of lines as the input.\n",
    "                  Output only the translation, with no additional text.\n",
    "                  INPUT:\n",
    "                  {batch}\n",
    "                  OUTPUT:\n",
    "                  '''\n",
    "\n",
    "        # 2.4.5: Set input parameters\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # 2.4.6: Move model and inputs to device before generating\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # 2.4.7: Set output parameters; temperature changes for each output\n",
    "        outputs = model.generate(**inputs, \n",
    "                                 max_new_tokens=1024,\n",
    "                                 temperature=float(sys.argv[1])\n",
    "        )\n",
    "\n",
    "        # 2.4.8: Decode output and generate translation\n",
    "        translation = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[-1]:], \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        # 2.4.9: Write to file\n",
    "        f.write(translation + '\\n')\n",
    "print(f'Translations successfully executed and saved to: {out_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e26d8",
   "metadata": {},
   "source": [
    "#### Part 3: Evaluate\n",
    "___\n",
    "The following chunks of code produce multi-reference BLEU score, and runs a Reward Model with the NMT & LLM translations as input preference-pair.\n",
    "* __Part 3.1__: Setup the BLEU score script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a83d15",
   "metadata": {
    "tags": [
     "get BLEU"
    ]
   },
   "outputs": [],
   "source": [
    "# 3.1.0: Set variables\n",
    "prfx = sys.argv[1]\n",
    "temp = sys.argv[2]\n",
    "split = f'{prfx}_{temp}'\n",
    "cand_path = f'translations/{split}Latn.de'\n",
    "refs_path = 'paraphrases/'\n",
    "out_dir = f'scores/'\n",
    "\n",
    "# 3.1.1: Load candidate text\n",
    "with open(cand_path, 'r', encoding='utf-8') as f:\n",
    "    cand = [line for line in f]\n",
    "\n",
    "# 3.1.2: Load reference texts\n",
    "refs = list()\n",
    "for file in os.listdir(refs_path):\n",
    "    if file.startswith('deu_Latn'):\n",
    "        with open(os.path.join(refs_path, file), 'r', encoding='utf-8') as f:\n",
    "            refs.append([line for line in f])\n",
    "\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists. Proceeds as normal.')\n",
    "    pass\n",
    "\n",
    "# 3.1.3: Store the multi-reference BLEU score\n",
    "with open(f'{out_dir}/{split.upper()}scores.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, r in enumerate(refs):\n",
    "        ind_score = sacrebleu.corpus_bleu(cand, [r])\n",
    "        f.write(f'BLEU score {i+1}: {ind_score}\\n')\n",
    "    tot_score = sacrebleu.corpus_bleu(cand, refs)\n",
    "    f.write(f'Total BLEU score: {tot_score}')\n",
    "\n",
    "print(f'\\nBLEU score successfully computed!\\nCheck \"{split.upper()}score.txt\" for results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87421c",
   "metadata": {},
   "source": [
    "* __Part 3.2__: Load the Reward Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5236db",
   "metadata": {
    "tags": [
     "load RM"
    ]
   },
   "outputs": [],
   "source": [
    "# WIP!!!!\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "model_name = 'Skywork/Skywork-Reward-V2-Llama-3.1-8B'\n",
    "rm = amfsc.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    attn_implementation='flash_attention_2',\n",
    "    num_labels=1\n",
    ")\n",
    "tokenizer = at.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47116345",
   "metadata": {},
   "source": [
    "* __Part 3.3__: Execute the Reward Model and save preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea4532",
   "metadata": {
    "tags": [
     "run RM"
    ]
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
