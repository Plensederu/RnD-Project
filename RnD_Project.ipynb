{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78583ea",
   "metadata": {},
   "source": [
    "### Research \\& Development Project\n",
    "___\n",
    "This is the step-by-step guide to what I did during the project. Following these steps will provide the same results\n",
    "at which I arrived.<br>\n",
    "It should be noted that in order for the shell script to properly work, all chunks of code post Part 2 must be tagged\n",
    "accordingly. <br>\n",
    "See '#CREATE TEMP PATHS' in the shell script for more information. Alternatively, download this notebook and take note of the tags for each cell.\n",
    "#### Part 1: Setup\n",
    "* __Step 1.1__: Setup necessary parameters, create the virtual environment, install necessary packages.<br>\n",
    "Once this is done, set the kernel to said environment.<br>\n",
    "For this to work, follow the instructions in the README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a3ad3",
   "metadata": {},
   "source": [
    "* __Step 1.2__: Retrieve and convert OpenNMT's NLLB transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d795252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the model from https://www.opennmt.net/Models-py/\n",
    "!wget https://s3.amazonaws.com/opennmt-models/nllb-200/nllb-200-3.3B-onmt.pt\n",
    "!wget https://s3.amazonaws.com/opennmt-models/nllb-200/flores200_sacrebleu_tokenizer_spm.model\n",
    "!mkdir nmt_resources paraphrases\n",
    "!unzip paraphrases.zip paraphrases\n",
    "!mv paraphrases.zip nmt_resources\n",
    "!mv nllb-200-3.3B-onmt.pt flores200_sacrebleu_tokenizer_spm.model nmt_resources/\n",
    "!ct2-opennmt-py-converter --model nmt_resources/nllb-200-3.3B-onmt.pt --quantization int8 --output_dir nmt_resources/nllb-200-3.3B-int8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b2ac7",
   "metadata": {},
   "source": [
    "Running the following cell will execute the entire script from start till finish. <br>\n",
    "There is no need to run cells manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a53c78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream"
    }
   ],
   "source": [
    "!sbatch execute.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c6a474",
   "metadata": {},
   "source": [
    "#### Part 2: Translate\n",
    "___\n",
    "The following chunks of code will load the models, translate the source text, and store the translated target text.\n",
    "* __Step 2.1__: Load & run NMT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd807a6e",
   "metadata": {
    "tags": [
     "NMT"
    ]
   },
   "outputs": [],
   "source": [
    "import ctranslate2\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#--------------------LOAD NMT--------------------#\n",
    "''' Test for GPU and load translator '''\n",
    "\n",
    "# 2.1.0: Set device to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2.1.1: Instantiate the translator\n",
    "ct_model_path = 'nmt_resources/nllb-200-3.3B-int8'\n",
    "sp_model_path = 'nmt_resources/flores200_sacrebleu_tokenizer_spm.model'\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(sp_model_path)\n",
    "\n",
    "# 2.1.2: [OPTIONAL]\n",
    "print(ctranslate2.__version__)\n",
    "try:\n",
    "    resrv_mem = torch.cuda.memory.memory_reserved(0)\n",
    "    alloc_mem = torch.cuda.memory.memory_allocated(0)\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    print(f\"\\n GPU available: {torch.cuda.is_available()}\\n\",\n",
    "          f\"Device name: {torch.cuda.get_device_name(0)}\\n\",\n",
    "          f\"Total memory on device: {total_mem / 1024**3:.2f} GB\\n\",\n",
    "          f\"Memory on device currently reserved (set aside): {resrv_mem / 1024**3:.2f} GB\\n\",\n",
    "          f\"Memory on device currently allocated (in use): {alloc_mem / 1024**3:.2f} GB\\n\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "#--------------------RUN NMT--------------------#\n",
    "''' Encode input, translate source text, decode output '''\n",
    "\n",
    "# 2.1.3 Define source and target languages\n",
    "src_lang = 'eng_Latn'\n",
    "tgt_lang = 'deu_Latn'\n",
    "\n",
    "# 2.1.4: Define paths\n",
    "inp_path = f'paraphrases/eng_Latn{sys.argv[1]}.txt'\n",
    "out_path = f'translations/nmt_Latn{sys.argv[1]}.de'\n",
    "\n",
    "beam_size = 4\n",
    "\n",
    "# 2.1.5: Open and read input file; return lines\n",
    "with open(inp_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 2.1.6: Define source and target prefixes\n",
    "source_sents = [line.strip() for line in lines]\n",
    "print(src_lang, source_sents[0], sep=' --> ')\n",
    "target_prefix = [[tgt_lang]] * len(source_sents)\n",
    "\n",
    "# 2.1.7: Subword source sentences\n",
    "source_sents_subword = sp.encode_as_pieces(source_sents)\n",
    "source_sents_subword = [[src_lang] + sent + ['</s>'] for sent in source_sents_subword]\n",
    "\n",
    "# 2.1.8: Translate source sentences\n",
    "translator = ctranslate2.Translator(ct_model_path, device=device)\n",
    "translations = translator.translate_batch(source_sents_subword, \n",
    "                                          batch_type='tokens',\n",
    "                                          max_batch_size=2048,\n",
    "                                          beam_size=beam_size,\n",
    "                                          target_prefix=target_prefix)\n",
    "translations = [translation.hypotheses[0] for translation in translations]\n",
    "\n",
    "# 2.1.9: Desubword target sentences\n",
    "translations_desubword = sp.decode(translations)\n",
    "translations_desubword = [sent[len(tgt_lang):].strip() for sent in translations_desubword]\n",
    "print(tgt_lang, translations_desubword[0], sep=' --> ')\n",
    "\n",
    "# 2.1.10: Create target directory\n",
    "os.makedirs('translations', exist_ok=True)\n",
    "\n",
    "# 2.1.11: Write translation to output file\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for line in translations_desubword:\n",
    "        f.write(line.strip() + '\\n')\n",
    "print(f'Translations successfully executed and saved to: {out_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25685d15",
   "metadata": {},
   "source": [
    "* __Part 2.2__: Load & run LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f437c5f",
   "metadata": {
    "tags": [
     "LLM"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  as at\n",
    "\n",
    "#--------------------LOAD LLM--------------------#\n",
    "''' Test for GPU and load the model '''\n",
    "\n",
    "# 2.2.0: Set device, dtype and device map to GPU if cuda == True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_dtype = torch.float16 if device == 'cuda' else torch.float32\n",
    "device_map = 'auto' if device == 'cuda' else None\n",
    "\n",
    "# 2.2.1: Instantiate tokenizer and model\n",
    "tokenizer = at.from_pretrained('meta-llama/Llama-2-13b-chat-hf')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-13b-chat-hf', \n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "# 2.2.2: [OPTIONAL]\n",
    "try:\n",
    "    resrv_mem = torch.cuda.memory.memory_reserved(0)\n",
    "    alloc_mem = torch.cuda.memory.memory_allocated(0)\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    print(f\"\\n GPU available: {torch.cuda.is_available()}\\n\",\n",
    "          f\"Device name: {torch.cuda.get_device_name(0)}\\n\",\n",
    "          f\"Total memory on device: {total_mem / 1024**3:.2f} GB\\n\",\n",
    "          f\"Memory on device currently reserved (set aside): {resrv_mem / 1024**3:.2f} GB\\n\",\n",
    "          f\"Memory on device currently allocated (in use): {alloc_mem / 1024**3:.2f} GB\\n\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "#--------------------RUN LLM--------------------#\n",
    "''' Formalize prompt, set temperature, translate source text '''\n",
    "\n",
    "# 2.2.3: Define paths\n",
    "inp_path = f'paraphrases/eng_Latn{sys.argv[3]}.txt'\n",
    "out_path = f'translations/llm_t{sys.argv[2]}_raw_Latn{sys.argv[3]}.de'\n",
    "\n",
    "# 2.2.4: Load source text\n",
    "with open(inp_path, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "\n",
    "# 2.2.5: Create target directory\n",
    "os.makedirs('translations', exist_ok=True)\n",
    "\n",
    "# 2.2.6: Provide prompt to model\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(0, len(lines)):\n",
    "        messages = [\n",
    "            {'role': 'user', 'content': f'Translate the following text from English to German: {lines[i]}'}\n",
    "            ]\n",
    "\n",
    "        # 2.2.7: Set input parameters\n",
    "        inputs = tokenizer.apply_chat_template(messages, \n",
    "                                               add_generation_prompt=False,\n",
    "                                               tokenize=True,\n",
    "                                               return_dict=True,\n",
    "                                               return_tensors='pt')\n",
    "        \n",
    "        # 2.2.8: Move inputs to device before generating\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # 2.2.9: Set output parameters; temperature changes for each output\n",
    "        max_new_tokens = int(len(tokenizer(lines[i]).input_ids) * 1.5) + 25\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=max_new_tokens,\n",
    "                                 temperature=float(sys.argv[1])\n",
    "        )\n",
    "\n",
    "        # 2.2.10: Decode output and generate translation\n",
    "        translation = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], \n",
    "                                       skip_special_tokens=True).strip()\n",
    "\n",
    "        # 2.2.11: Write to file\n",
    "        f.write(translation + '\\n')\n",
    "print(f'Translations successfully executed and saved to: {out_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39399dd2",
   "metadata": {},
   "source": [
    "* __Part 2.3__: Clean LLM translations from noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ad9d9",
   "metadata": {
    "tags": [
     "NOISE"
    ]
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "class CleanNoise:\n",
    "    def __init__(self, path, inp_file, out_file, new_subdir, old_subdir):\n",
    "        self.path = path\n",
    "        self.inp_file = inp_file\n",
    "        self.out_file = out_file\n",
    "        self.new_subdir = new_subdir\n",
    "        self.old_subdir = old_subdir\n",
    "\n",
    "    # 2.3.1: Get the raw translated lines as a list\n",
    "    def get_lines(self):\n",
    "        with open(os.path.join(self.old_subdir, self.inp_file), 'r', encoding='utf-8') as f:\n",
    "            return f.readlines()\n",
    "\n",
    "    # 2.3.2: Ignore blank spaces produced by the LLM\n",
    "    def ignore_blank(self):\n",
    "        return [line.strip() for line in self.get_lines() if not line.isspace()]\n",
    "\n",
    "    # 2.3.3: Detect german lines and ignore overtly English noise by the LLM\n",
    "    def find_deu(self):\n",
    "        deu = []\n",
    "        for line in self.ignore_blank():\n",
    "            try:\n",
    "                if detect(line) == 'de':\n",
    "                    deu.append(line)\n",
    "            except LangDetectException:\n",
    "                pass\n",
    "        return deu\n",
    "    \n",
    "    # 2.3.4: Filter non-overt LLM noise such as unfinished notes\n",
    "    def filter_noise(self):\n",
    "        pattern = r'(\\*)'\n",
    "        noise = []\n",
    "        for line in self.find_deu():\n",
    "            if not re.search(pattern, line, re.IGNORECASE):\n",
    "                noise.append(line)\n",
    "        return noise\n",
    "    \n",
    "    # 2.3.5: Save filtered german lines to new file path\n",
    "    def save(self):\n",
    "        os.makedirs(self.new_subdir, exist_ok=True)   # make folder for filtered translations without noise\n",
    "        with open(os.path.join(self.new_subdir, self.out_file), 'w', encoding='utf-8') as f:\n",
    "            for line in self.filter_noise():\n",
    "                f.write(line + '\\n')\n",
    "\n",
    "    # 2.3.6: Move old raw translations to new subdirectory\n",
    "    def move(self):\n",
    "        os.makedirs(self.old_subdir, exist_ok=True)   # make folder for raw translations with noise\n",
    "        try:\n",
    "            for file in os.listdir(self.path):\n",
    "                if file.startswith('llm'):\n",
    "                    shutil.move(file, self.old_subdir + file)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# 2.3.7: Define paths\n",
    "path = 'translations/'\n",
    "inp_file = f'llm_{sys.argv[1]}_raw_Latn{sys.argv[2]}.de'\n",
    "out_file = f'llm_{sys.argv[1]}_filtered_Latn{sys.argv[2]}.de'\n",
    "new_subdir = f'translations/filtered_translations/'\n",
    "old_subdir = f'translations/raw_translations/'\n",
    "\n",
    "# 2.3.8: Instantiate and call class\n",
    "cn = CleanNoise(inp_file=inp_file, \n",
    "                out_file=out_file,\n",
    "                new_subdir=new_subdir,\n",
    "                old_subdir=old_subdir)\n",
    "cn.move()\n",
    "cn.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95addf",
   "metadata": {},
   "source": [
    "* __Part 2.4__: Embedd all translated texts.<br>\n",
    "_Make semantic embeddings for all translations and select 1800 sentences with similar embeddings._ <br>\n",
    "_This is to make sure that all translations are sentence-aligned and are ready for evaluation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacb0a8",
   "metadata": {
    "tags": [
     "SELECT"
    ]
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 2.4.0: Define paths\n",
    "os.makedirs('aligned/', exist_ok=True)\n",
    "\n",
    "if sys.argv[1] == 'llm':                                            # Candidate will be LLM translations\n",
    "    cand_file = f'llm_{sys.argv[2]}_filtered_Latn{sys.argv[3]}.de'\n",
    "    cand_path = f'translations/filtered_translations/{cand_file}'\n",
    "    new_cand = f'aligned/llm_{sys.argv[2]}_Latn-aligned{sys.argv[3]}.de'\n",
    "elif sys.argv[1] == 'ref':                                          # Candidate will be paraphrased references\n",
    "    cand_file = f'deu_Latn_{sys.argv[2]}.txt'\n",
    "    cand_path = f'paraphrases/{cand_file}'\n",
    "    new_cand = f'aligned/deu_Latn-aligned_{sys.argv[2]}.txt'\n",
    "\n",
    "nmt_file = f'nmt_Latn{sys.argv[3]}.de'\n",
    "nmt_path = f'translations/{nmt_file}'\n",
    "new_nmt = 'aligned/nmt_Latn-aligned.de'\n",
    "\n",
    "# 2.4.1: Load Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2.4.2: Load sentences to encode as list of strings\n",
    "with open(nmt_path, 'r', encoding='utf-8') as nmt, open(cand_path, 'r', encoding='utf-8') as cand:\n",
    "    nmt_translations = nmt.readlines()\n",
    "    candidate_txt = cand.readlines()\n",
    "\n",
    "# 2.4.3: Encode sentences by computing embeddings\n",
    "nmt_emb = model.encode(nmt_translations[:1900])\n",
    "cand_emb = model.encode(candidate_txt)\n",
    "\n",
    "# 2.4.4: Semantic search for each NMT sentence; find best matching candidate sentence\n",
    "matches = util.semantic_search(nmt_emb, cand_emb, top_k=1)\n",
    "\n",
    "# 2.4.5: Align matching sentences and store in list\n",
    "aligned = list()\n",
    "for nmt_idx, match_lst in enumerate(matches):\n",
    "    cand_idx = match_lst[0]['corpus_id']\n",
    "    aligned.append((nmt_translations[nmt_idx], candidate_txt[cand_idx]))\n",
    "\n",
    "# 2.4.6: Save aligned sentences to separate files ready for evaluation\n",
    "with open(new_nmt, 'w', encoding='utf-8') as f, open(new_cand, 'w', encoding='utf-8') as c:\n",
    "    for line in aligned:\n",
    "        f.write(line[0])\n",
    "        c.write(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8e26d8",
   "metadata": {},
   "source": [
    "#### Part 3: Evaluate\n",
    "___\n",
    "The following chunks of code produce multi-reference BLEU score, and runs a Reward Model with the NMT & LLM translations as input preference-pair.\n",
    "* __Part 3.1__: Setup the BLEU score script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a83d15",
   "metadata": {
    "tags": [
     "BLEU"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sacrebleu\n",
    "\n",
    "# 3.1.0: Set variables\n",
    "prfx = sys.argv[1]\n",
    "temp = sys.argv[2]\n",
    "mask = sys.argv[3]\n",
    "split = f'{prfx}_{temp}'\n",
    "cand_path = f'aligned/{split}Latn-aligned{mask}.de'\n",
    "refs_path = 'aligned/'\n",
    "out_dir = f'scores/'\n",
    "\n",
    "# 3.1.1: Load candidate text\n",
    "with open(cand_path, 'r', encoding='utf-8') as f:\n",
    "    cand = [line for line in f]\n",
    "\n",
    "# 3.1.2: Load reference texts\n",
    "refs = list()\n",
    "for file in os.listdir(refs_path):\n",
    "    if file.startswith('deu_Latn'):\n",
    "        with open(os.path.join(refs_path, file), 'r', encoding='utf-8') as f:\n",
    "            refs.append([line for line in f])\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 3.1.3: Store the individual and multi-reference BLEU scores to separate .txt files\n",
    "with open(f'{out_dir}/{split.lower()}scores{mask}.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, r in enumerate(refs):\n",
    "        ind_score = sacrebleu.corpus_bleu(cand, [r])\n",
    "        f.write(f'BLEU score {i+1}: {ind_score}\\n')\n",
    "    tot_score = sacrebleu.corpus_bleu(cand, refs)\n",
    "    f.write(f'Total BLEU score: {tot_score}')\n",
    "\n",
    "print(f'\\nBLEU score successfully computed!\\nCheck \"{split.upper()}score.txt\" and \"BLEU_scores.csv\" for results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c8ae4",
   "metadata": {
    "tags": [
     "CSV"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "path = 'scores/'\n",
    "bleu = []\n",
    "temp = []\n",
    "nmt = []\n",
    "\n",
    "# 3.1.4: Find all the multi-reference BLEU scores among LLM outputs\n",
    "for score in sorted(os.listdir(path)):\n",
    "    if score.startswith('llm'):\n",
    "        with open(os.path.join(path, score), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[4]\n",
    "            bleu.append(lines[25:30].strip())\n",
    "    if score.startswith('nmt'):\n",
    "        with open(os.path.join(path, score), 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[4]\n",
    "            nmt.append(lines[25:30].strip())\n",
    "\n",
    "# 3.1.5: Loop over temperatures\n",
    "for i in range(9):\n",
    "    t = f'{i*0.2:.2f}'\n",
    "    temp.append(t)\n",
    "\n",
    "# 3.1.6: Store the multi-reference BLEU scores for LLM to a shared .csv file\n",
    "with open(f'{path}/BLEU_scores{sys.argv[1]}.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['temp','bleu','nmt'])\n",
    "    w.writerows(zip(temp, bleu, nmt * len(temp)))   # This is a terrible solution, but I don't know how else to make it work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef8d97",
   "metadata": {},
   "source": [
    "* __Part 3.2__: Plot the LLM temperature-BLEU score regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7627f08",
   "metadata": {
    "tags": [
     "PLOT"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 3.2.1: Read CSV datasets for pre- and post-masking\n",
    "df_a = pd.read_csv('scores/BLEU_scores.csv')\n",
    "df_b = pd.read_csv('scores/BLEU_scores-mask.csv')\n",
    "\n",
    "# 3.2.2: Set idx to temperature and rename bleu column\n",
    "df_a = df_a.set_index('temp').rename(columns={'bleu': 'LLM Translations'})\n",
    "df_b = df_b.set_index('temp').rename(columns={'bleu': 'LLM Translations'})\n",
    "\n",
    "# 3.2.3: Create subplot using two axes (share y-axis and set subplot horizontally)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "# 3.2.4: Pre-mask subplot\n",
    "axes[0].bar(df_a.index.astype(str), df_a['LLM Translations'], label='LLM Translations')\n",
    "axes[0].bar(['NMT'], [df_a['nmt'].iloc[0]], label='NMT Translation')\n",
    "axes[0].set_title('Pre-Masking')\n",
    "axes[0].set_xlabel('Temperature / Model')\n",
    "axes[0].set_ylabel('BLEU score')\n",
    "axes[0].legend()\n",
    "\n",
    "# 3.2.5: Post-mask subplot\n",
    "axes[1].bar(df_b.index.astype(str), df_b['LLM Translations'], label='LLM Translations')\n",
    "axes[1].bar(['NMT'], [df_b['nmt'].iloc[0]], label='NMT Translation')\n",
    "axes[1].set_title('Post-Masking')\n",
    "axes[1].set_xlabel('Temperature / Model')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3.2.6: Save plot\n",
    "plt.savefig('bleu-histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87421c",
   "metadata": {},
   "source": [
    "* __Part 3.3__: Load & run RM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5236db",
   "metadata": {
    "tags": [
     "RM"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from transformers import AutoModelForSequenceClassification as amfsc, AutoTokenizer as at\n",
    "\n",
    "# 3.3.0: Define paths\n",
    "os.makedirs('reward_scores/', exist_ok=True)\n",
    "inp_path = 'aligned/'\n",
    "out_path = 'reward_scores/'\n",
    "llm_file = f'llm_{sys.argv[1]}_Latn-aligned{sys.argv[2]}.de'\n",
    "nmt_file = f'nmt_Latn-aligned{sys.argv[2]}.de'\n",
    "\n",
    "# 3.3.1: Load Reward model and tokenizer\n",
    "model_name = 'Skywork/Skywork-Reward-V2-Llama-3.1-8B'\n",
    "rm = amfsc.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    num_labels=1\n",
    ")\n",
    "tokenizer = at.from_pretrained(model_name)\n",
    "\n",
    "# 3.3.3: Load preference pairs\n",
    "with open(os.path.join(inp_path, llm_file), 'r') as llm, open(os.path.join(inp_path, nmt_file), 'r') as nmt:\n",
    "    prefs = []\n",
    "    l_lines = llm.readlines()\n",
    "    n_lines = nmt.readlines()\n",
    "    n = max(len(n_lines), len(l_lines))\n",
    "    for i in range(n):\n",
    "        nmt_sent = n_lines[i].strip()\n",
    "        llm_sent = l_lines[i].strip()\n",
    "\n",
    "        # 3.3.4: Define model inputs\n",
    "        nmt_inp = tokenizer(nmt_sent, return_tensors='pt').to(device)\n",
    "        llm_inp = tokenizer(llm_sent, return_tensors='pt').to(device)\n",
    "\n",
    "        # 3.3.5: Model inference        \n",
    "        with torch.no_grad():\n",
    "            score_nmt = rm(**nmt_inp).logits[0][0].item()\n",
    "            score_llm = rm(**llm_inp).logits[0][0].item()\n",
    "        if score_llm > score_nmt:\n",
    "            prefs.append('llm')\n",
    "        elif score_nmt > score_llm:\n",
    "            prefs.append('nmt')\n",
    "        else:\n",
    "            prefs.append('equal')\n",
    "\n",
    "# 3.3.6: Save outputs\n",
    "with open(os.path.join(out_path, f'{sys.argv[2]}Preference_points-{sys.argv[1]}.txt'), 'w', encoding='utf-8') as f:\n",
    "    for pref in prefs:\n",
    "        f.write(pref + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d09d6",
   "metadata": {},
   "source": [
    "* __Part 3.4__: Get the total Reward Model preferences over the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded6b30",
   "metadata": {
    "tags": [
     "TOTAL"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = 'reward_scores/'\n",
    "out_file = f'{sys.argv[1]}Total_Reward_Score.txt'\n",
    "\n",
    "pref_files = sorted(file for file in os.listdir(path) if file.startswith('Preference'))\n",
    "\n",
    "with open(os.path.join(path, out_file), 'w', encoding='utf-8') as f_write:\n",
    "    for idx, file in enumerate(pref_files):\n",
    "        llm = 0\n",
    "        nmt = 0\n",
    "        equal = 0\n",
    "        if file.startswith(f'{sys.argv[1]}Preference'):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f_read:\n",
    "                lines = f_read.readlines()\n",
    "                for line in lines:\n",
    "                    if line.strip() == 'nmt':\n",
    "                        nmt += 1\n",
    "                    elif line.strip() == 'llm':\n",
    "                        llm += 1\n",
    "                    elif line.strip() == 'equal':\n",
    "                        equal += 1\n",
    "        f_write.write(f'''Comparing NMT translations with with LLM translations at temperature {idx*.2:.2f} yielded the following:\\\n",
    "                \\nLLM: {llm}\\nNMT: {nmt}\\nEQUAL: {equal}\\nHence, the model preferred the LLM {llm}/{llm+nmt} (ignoring equals) \\n\\n''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c7341",
   "metadata": {},
   "source": [
    "#### Part 4: Mask (Perturbation)\n",
    "___\n",
    "The following chunk of code is used to mask the source text before being run through the translational pipeline again.\n",
    "* __Part 4.1__: Use Named Entity Recognition (NER) to mask target text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fd1bc",
   "metadata": {
    "tags": [
     "MASK"
    ]
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "# 4.1.0: Define paths\n",
    "path = 'paraphrases/'\n",
    "in_file = 'eng_Latn.txt'\n",
    "out_file = 'eng_Latn-mask.txt'\n",
    "\n",
    "# 4.1.1: Load pre-trained NER model\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "# 4.1.2: Mask source text and save to clean file\n",
    "with open(os.path.join(path, in_file), 'r') as source, open(os.path.join(path, out_file), 'w') as target:\n",
    "    lines = source.readlines()\n",
    "    for line in lines:\n",
    "        doc = nlp(line)\n",
    "        masked_txt = line\n",
    "        for ent in reversed(doc.ents):\n",
    "            start, end = ent.start_char, ent.end_char\n",
    "            masked_txt = masked_txt[:start] + '[MASK]' + masked_txt[end:]\n",
    "        target.write(masked_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d131a9",
   "metadata": {},
   "source": [
    "#### Part 5: Test Statistical Significance (Optional)\n",
    "___\n",
    "* __Part 5.1__: The following chunk of code computes p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517537ef",
   "metadata": {
    "tags": [
     "SIGNIFICANCE"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binomtest\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 5.1.0: Define paths\n",
    "path = 'reward_scores/'\n",
    "out_file = f'{sys.argv[1]}statistical_significance.txt'\n",
    "\n",
    "# 5.1.1: Sort files in path directory\n",
    "pref_files = sorted(file for file in os.listdir(path) if file.startswith('Preference'))\n",
    "\n",
    "# 5.1.2: Store distribution as tuples in dict with temp as keys\n",
    "with open(os.path.join(path, out_file), 'w', encoding='utf-8') as f_write:\n",
    "    data = defaultdict(tuple)\n",
    "    for idx, file in enumerate(pref_files):\n",
    "        llm = 0\n",
    "        nmt = 0\n",
    "        if file.startswith(f'{sys.argv[1]}Preference'):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f_read:\n",
    "                lines = f_read.readlines()\n",
    "                for line in lines:\n",
    "                    if line.strip() == 'llm':\n",
    "                        llm += 1\n",
    "                    elif line.strip() == 'nmt':\n",
    "                        nmt += 1\n",
    "        if idx == 0:\n",
    "            temp = f't{idx:02d}'\n",
    "        else:\n",
    "            temp = f't{idx*2:02d}'\n",
    "        data[temp] = (llm, nmt)\n",
    "\n",
    "    # 5.1.3: Loop over temperatures (p-value for each) and save to clean file\n",
    "    for k, (llm, nmt) in data.items():\n",
    "        n = llm + nmt\n",
    "        p = binomtest(nmt, n=n, p=0.5).pvalue\n",
    "        f_write.write(f'p-value for {k} = {p:.2e}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
